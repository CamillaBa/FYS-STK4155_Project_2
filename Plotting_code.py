import scipy.sparse as sp
from Methods import *
from regdata import *
import pickle, os
import numpy as np
from timeit import default_timer as timer
from math import ceil

#==================================================================================================================
# Below are two functions used to save data from performing newton raphson and stochastic gradient descent
# on training data.
#==================================================================================================================

def generate_logreg_NR_betas(folder_name,train_X,train_Y,design_row,number_iterations=3):
    print("Initiating training data object.")
    data = regdata(train_X,train_Y,design_row)

    # load last betas_NR if they exists within folder
    if os.path.isfile("./{}/logreg/NR_betas.pickle".format(folder_name)) == False:
        betas_NR = []
        beta_NR = None
    else:
        betas_NR = load_object_from_file("./{}/logreg/NR_betas.pickle".format(folder_name))
        beta_NR = betas_NR[-1]

    # update using Newton-Rahspons algorithm
    for iteration in range(0,number_iterations):
        beta_NR = np.copy(data.beta_NR(beta_init=beta_NR, epsilon = 1e-8, max_number_iterations=1))
        betas_NR.append(beta_NR)
        print("Completed iteration {}".format(iteration))

    # save the coefficients to file
    save_object_to_file(betas_NR,"./{}/logreg/NR_betas.pickle".format(folder_name))

def generate_logreg_SGD_betas(folder_name,train_X,train_Y,design_row,
                              number_epochs=20,
                              number_batches_list = [1,2,3,4,5],
                              t0=1,
                              t1=1):
    print("Initiating training data object.")
    data = regdata(train_X,train_Y,design_row)
    data.learn.t0=t0
    data.learn.t1=t1

    # load last betas_SGD if they exists within folder
    if os.path.isfile("./{}/logreg/SGD_betas.pickle".format(folder_name)) == True:
        betas= load_object_from_file("./{}/logreg/SGD_betas.pickle".format(folder_name))
        beta={}
        for M in number_batches_list:
            beta[str(M)] = betas[str(M)][-1]
    else:
        betas = {}
        beta= {}
        for M in number_batches_list:
            beta[str(M)]=np.copy(data.log_beta_SGD(epochs = 0)) # load default beta
            betas[str(M)]=[beta[str(M)]] # append beta to list

    # make a few iterations through the various algorithm (few = number_iteraions)
    for epoch in range(0,number_epochs):
        true_epoch = epoch + len(betas[str(number_batches_list[0])])
        for M in number_batches_list:
            # update using stochastic gradient descent with M batches
            beta[str(M)] = np.copy(data.log_beta_SGD(beta_init=beta[str(M)],
                                                     epoch_start = true_epoch,
                                                     epochs = 1,
                                                     M=M))
            betas[str(M)].append(beta[str(M)])
        print("Completed epoch {}".format(epoch))
    
    # save the coefficients to file
    save_object_to_file(betas,"./{}/logreg/SGD_betas.pickle".format(folder_name))

#==================================================================================================================
# Below are two functions to make plots of data generated by the two functions above.
#==================================================================================================================

def generate_logreg_NR_plot(folder_name,train_X,train_Y,test_X,test_Y,design_row):
    print("Initiating testing data object.")
    data_test = regdata(test_X,test_Y,design_row)

    print("Generating plot.")
    betas_NR = load_object_from_file("./{}/logreg/NR_betas.pickle".format(folder_name))

    # initiate empy array for accuracies for Newton Raphson data
    n = len(betas_NR)
    accuracies_NR = np.zeros(n)

    # load betas and calculate accuracies
    for iteration in range(n):
        model_test_Y = data_test.model(betas_NR[iteration],method_name='log')
        accuracies_NR[iteration] =  accuracy(test_Y,model_test_Y)

    # create figure Newton-Raphson
    plt.figure("Logistic regression (Newton-Raphson)")
    plt.plot(accuracies_NR,"o--")
    plt.title("Logistic regression (Newton-Raphson) \n |test data|={}, |train data|={}".format(len(test_X),len(train_X)))
    plt.xlabel("Iterations")
    plt.ylabel("Accuracy score on testing data")
    plt.show(block=False)

def generate_logreg_SGD_plot(folder_name,train_X,train_Y,test_X,test_Y,design_row,
                         number_batches_list = [1,2,3,4,5],
                         t0=1,t1=1):
    print("Initiating testing data object.")
    data_test = regdata(test_X,test_Y,design_row)

    print("Generating plot.")
    betas_SGD = load_object_from_file("./{}/logreg/SGD_betas.pickle".format(folder_name))


    # initiate dictionary with empy arrays to hold accuracies for SGD data
    m = len( betas_SGD[ str( number_batches_list[0])])
    accuracies_SGD = {}
    for M in number_batches_list:
        accuracies_SGD[str(M)]=np.zeros(m)

    # load betas and calculate accuracies
    for epoch in range(m):
        for M in number_batches_list:
            model_test_Y = data_test.model(betas_SGD[str(M)][epoch],method_name='log')
            accuracies_SGD[str(M)][epoch] = accuracy(test_Y,model_test_Y)


    # print settings for best model to terminal
    best_acc = max([accuracies_SGD[str(m)][-1] for m in number_batches_list])
    best_number_batches = 0
    for m in  number_batches_list:
        if accuracies_SGD[str(m)][-1]==best_acc:
            best_number_batches = m

    print("Accuracies logistic regression:")
    print("Accuracy final epoch (on training data): {}".format(best_acc))
    print("Best number of batches: {}".format(best_number_batches))

    #print("Accuracies logistic regression:")
    #print("Accuracy final epoch (on training data): {}".format(accuracies_SGD[str(1)][-1]))

    # create figure SGD
    plt.figure("Logistic regression (SGD)")
    plt.title("Logistic regression (SGD) \n |test data|={}, |train data|={}\n".format(len(test_X),len(train_X))+r"t_0="+str(t0)+r", t_1="+str(t1))
    for M in number_batches_list:
        plt.plot(accuracies_SGD[str(M)],'o--',label="#batches = {}".format(M))
    plt.xlabel("epochs")
    plt.ylabel("accuracy score on testing data")
    plt.legend(loc='best')
    plt.show(block=False)  

#==================================================================================================================
# Below is a function to plot the accuracy of stochastic gradient descent with a various number of batches vs time
#==================================================================================================================

def plot_acc_time_SGD(train_X,train_Y,
                      test_X,test_Y,
                      design_row,
                      number_batches_list=[1,2,3,4,5],
                      time_limit = 1,
                      t0=1,t1=0):
    data_train = regdata(train_X,train_Y,design_row)
    data_train.learn.t0=t0
    data_train.learn.t1=t1
    data_test = regdata(test_X,test_Y,design_row)
    print("Calculating plot")
    plt.figure("Logistic regression (SGD): accuracy vs time")
    plt.title("Logistic regression (SGD), \n |test data|={}, |train data|={} \n".format(len(test_X),len(train_X))+r"t_0="+str(t0)+r", t_1="+str(t1))
    for M in number_batches_list:
        epoch = 1
        excess_time=0
        time_list = []

        # initial accuracy
        beta = np.copy(data_train.log_beta_SGD(epochs = 0))
        beta_old = np.copy(beta)
        model_test_Y = data_test.model(beta,method_name='log')
        accuracies= [accuracy(test_Y,model_test_Y)]

        # timing time needed to compute new steps and saving accuracy scores
        start = timer()
        while timer()-start-excess_time <  time_limit:
            beta = np.copy(data_train.log_beta_SGD(beta_init=beta_old,
                                                   epoch_start = epoch,
                                                   epochs = 1,
                                                   M=M))
            # store time it takes to handle data and calculate accuracy score as excess_time
            T=timer(); time_list.append(T-excess_time)
            beta_old = np.copy(beta)
            epoch += 1
            model_test_Y = data_test.model(beta,method_name='log')
            accuracies.append(accuracy(test_Y,model_test_Y))
            excess_time += timer()-T
        time_list = [start] + time_list
        # normalize time list
        time_list = np.array(time_list)
        time_list = time_list - time_list[0]
        print("Completed calculation for {} batches.".format(M))
        plt.plot(time_list,accuracies,'o--',label = "# batches = {}, #epochs = {}".format(M,epoch))
    plt.xlabel("time [s]")
    plt.ylabel("accuracy score on testing data")
    plt.legend(loc='best')
    plt.show(block=False)


#==================================================================================================================
# Below are two functions to store data and plot accuracy of the implementation of stochastic gradient descent 
# for the MLP class
#==================================================================================================================

def generate_nn_SGD_accuracies_vs_epochs_file(folder_name,network_size,train_X,train_Y,test_X,test_Y,
                                              DC = 'default', A_DA = 'default',
                                              number_batches_list=[1,2,3,4,5],
                                              number_epochs=100,
                                              t0=1,t1=1):
    # initiate empty dictionary to store accuracies
    accuracies = {}
    for M in number_batches_list:
            accuracies[str(M)]=np.zeros(number_epochs)


    for M in number_batches_list:
        # seed to make sure the weights and biases start equally for all partitions into batches
        np.random.seed(10) 
        network = MLP(network_size, DC=DC, A_DA = A_DA)
        network.learn.t0=t0
        network.learn.t1=t1

        for epoch in range(0,number_epochs):
            model_test_Y = classifyer(network.model(test_X)) #model_test(network,test_X)
            accuracies[str(M)][epoch] =  accuracy(test_Y,model_test_Y)
            print("Completed epoch {}".format(epoch))
            network.train(train_X, train_Y, 
                          epoch_start = epoch, 
                          epochs = 1, 
                          number_batches = M)
    
    save_object_to_file(accuracies,"./{}/neural_network/accuracies.pickle".format(folder_name))

def plot_nn_SGD_accuracies_vs_epochs(folder_name,network_size,
                                     train_X,train_Y,
                                     test_X,test_Y,
                                     plot_title = '',
                                     number_batches_list=[1,2,3,4,5],
                                     t0=1,t1=1):
    # load data
    accuracies = load_object_from_file("./{}/neural_network/accuracies.pickle".format(folder_name))
   
    # print settings for best model to terminal
    best_acc = max([accuracies[str(m)][-1] for m in number_batches_list])
    best_number_batches = 0
    for m in  number_batches_list:
        if accuracies[str(m)][-1]==best_acc:
            best_number_batches = m

    print("Accuracies logistic regression:")
    print("Accuracy final epoch (on training data): {}".format(best_acc))
    print("Best number of batches: {}".format(best_number_batches))

    # make plot
    print("Calculating plot")
    plt.figure("Neural network (SGD)+{}".format(plot_title))
    plt.title(plot_title+" (MLP {})  \n |test data|={}, |train data|={} \n".format(network_size,len(test_X),len(train_X))+r"t_0="+str(t0)+r", t_1="+str(t1))
    for M in number_batches_list:
        plt.plot(accuracies[str(M)],'o--',label = "#batches = {}".format(M))
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy score on testing data")
    plt.legend(loc='best')
    plt.show(block=False)

#==================================================================================================================
# Neural network: accuracy vs time and R2 score vs time
#==================================================================================================================

# Note: The two functions below are suitable to be simplified to only one function.

def plot_nn_accuracies_time_batches(network_size,
                                    train_X,train_Y,
                                    test_X,test_Y,
                                    name = '',
                                    DC = 'default',
                                    A_DA = 'default',
                                    number_batches_list=[1,2,3,4,5],
                                    time_limit = 2,
                                    t0=1,t1=1):
    '''Accuracy vs time plot.'''

    print("Calculating plot...")
    plt.figure(name+"Neural network (SGD) accuracy vs time")
    plt.title(name+"(MLP {}),  \n |test data|={}, |train data|={} \n".format(network_size,len(test_X),len(train_X))+r"t_0="+str(t0)+r", t_1="+str(t1))
    for M in number_batches_list:
        # seed to make sure the weights and biases start equally for all partitions into batches
        np.random.seed(10)
        network =  MLP(network_size, DC = DC, A_DA = A_DA)
        network.learn.t0 = t0
        network.learn.t1 = t1

        model_test_Y = classifyer(network.model(test_X))
        accuracies = [accuracy(test_Y,model_test_Y)]
        epoch = 0
        excess_time=0
        start = timer()
        time_list = [start]
        while timer()-start-excess_time <  time_limit:
            network.train(train_X, train_Y, 
                          epoch_start = epoch, 
                          epochs = 1,
                          number_batches = M)
            # store time it takes to handle data and calculate accuracy score as excess_time
            T=timer(); time_list.append(T-excess_time)
            epoch += 1
            model_test_Y = classifyer(network.model(test_X))
            accuracies.append(accuracy(test_Y,model_test_Y))
            excess_time += timer()-T

        # normalize time list
        time_list = np.array(time_list)
        time_list = time_list - time_list[0]
        print("Completed calculation for {} batches.".format(M))
        plt.plot(time_list,accuracies,'o--',label = "# batches = {}, #epochs = {}".format(M,epoch))
    plt.xlabel("Time [s]")
    plt.ylabel("Accuracy score on testing data")
    plt.legend(loc='best')
    plt.xlim((0,time_limit))
    plt.show(block=False)

def plot_nn_R2_time_batches(network_size,
                            train_X,train_Y,
                            test_X,test_Y,
                            DC = 'default',
                            number_batches_list=[1,2,3,4,5],
                            time_limit = 2,
                            t0=1,t1=1):
    '''R2 vs time plot.'''

    print("Calculating plot...")
    plt.figure("Neural network (SGD) R2 vs time")
    plt.title("Neural network (SGD), \n shape= {},  \n |test data|={}, |train data|={} \n".format(network_size,len(test_X),len(train_X))+r"t_0="+str(t0)+r", t_1="+str(t1))
    for M in number_batches_list:
        # seed to make sure the weights and biases start equally for all partitions into batches
        np.random.seed(10)
        if DC == 'default':
            network =  MLP(network_size)
        else:
            network =  MLP(network_size, DC = DC)
        network.learn.t0 = t0
        network.learn.t1 = t1

        model_test_Y = network.model(test_X)
        R2_scores = [R2(test_Y,model_test_Y)]
        epoch = 0
        excess_time=0
        start = timer()
        time_list = [start]
        while timer()-start-excess_time <  time_limit:
            network.train(train_X, train_Y, 
                          epoch_start = epoch, 
                          epochs = 1,
                          number_batches = M)
            # store time it takes to handle data and calculate accuracy score as excess_time
            T=timer(); time_list.append(T-excess_time)
            epoch += 1
            model_test_Y = network.model(test_X)
            R2_scores.append(R2(test_Y,model_test_Y))
            excess_time += timer()-T

        # normalize time list
        time_list = np.array(time_list)
        time_list = time_list - time_list[0]
        print("Completed calculation for {} batches.".format(M))
        R2_scores = np.array(R2_scores)
        plt.plot(time_list,np.clip(R2_scores,0,1),'o--',label = "# batches = {}, #epochs = {}".format(M,epoch))
    plt.xlabel("Time [s]")
    plt.ylabel("R2 score on testing data")
    plt.legend(loc='best')
    plt.xlim((0,time_limit))
    plt.show()

#==================================================================================================================
# Below are two functions to generate data from k-fold cross validation of neural network regression, 
# and plot R2, MSE, Bias and Variance
#==================================================================================================================

def generate_kval_nn_data(filename,X,Y,network_size,
                          epochs = 10,
                          plot_step = 5,
                          k=5,
                          t0=1,t1=1):
    N = len(X)
    partition = get_partition(N,k)
    
    # creating empty dictionary to store data
    out = {}
    for score in ["R2","MSE","bias","var"]:
        for part in ["train","test"]:
            out[score+'_'+part]=np.zeros(ceil(epochs/plot_step)) # for example: out[R2_test] = np.zeros(epochs)

    for index, part in enumerate(partition):
        # divide incoming data into train and test data in accordence with input k
        test_X, test_Y = X[part], Y[part]
        part_complement = partition[:index]+partition[index+1:]
        part_complement = sum(part_complement,[])
        train_X, train_Y = X[part_complement], Y[part_complement]
        # initiate neural network
        network =  MLP(network_size)

        # begin training
        for epoch in range(epochs):
            print(epoch)
            if epoch % plot_step == 0:
                # get resulting models
                model_test_Y = np.copy(network.model(test_X))
                model_train_Y = np.copy(network.model(train_X))

                i = int(epoch/plot_step)

                # sum training scores of current epoch
                out['R2_train'][i] += R2(train_Y,model_train_Y)
                out['MSE_train'][i] += MSE(train_Y,model_train_Y)
                out['bias_train'][i] += bias(train_Y,model_train_Y)
                out['var_train'][i] += var(model_train_Y)
                

                # sum test scores of current epoch
                out['R2_test'][i] += R2(test_Y,model_test_Y)
                out['MSE_test'][i] += MSE(test_Y,model_test_Y)
                out['bias_test'][i] += bias(test_Y,model_test_Y)
                out['var_test'][i] += var(model_test_Y)

            # train model one epoch
            network.train(train_X, train_Y, epoch_start = epoch, epochs = 1)
            

    # take average of scores
    for score in ["R2","MSE","bias","var"]:
        for part in ["train","test"]:
            out[score+'_'+part]/=k
    
    # save data
    save_object_to_file(out,"./kval/neural_network/{}_{}val_plot_data.pickle".format(filename,k))
    save_object_to_file(partition,"./kval/neural_network/partition.pickle".format(filename,k))

def plot_error_kval_nn(plot_title,filename,data_size,network_size,
                       k=5, 
                       plot_step = 5,
                       t0=1,t1=1):
    plot_data = load_object_from_file("./kval/neural_network/{}_{}val_plot_data.pickle".format(filename,k))
    epochs = np.arange(0,plot_step*len(plot_data["R2_test"]),plot_step)


    print("R2 scores at final epoch:")
    print("R2 test: ", plot_data["R2_test"][-1])
    print("R2 train: ", plot_data["R2_train"][-1])

    # R2 score
    plt.figure("R2")
    plt.plot(epochs,np.clip(plot_data["R2_train"],0,1),'o-',label = 'train')
    plt.plot(epochs,np.clip(plot_data["R2_test"],0,1),'o--',label = 'test')
    plt.xlabel('epochs')
    plt.ylabel('$R^2$ score')
    plt.title(plot_title+" (MLP {} )\n |data| = {}, \n{}-fold cross validation".format(network_size,data_size,k))
    plt.legend(loc='best')
    plt.grid(True)
    plt.show(block=False)


    # MSE, bias, variance
    plt.figure("Error")
    for score in ["MSE","bias","var"]:
        plt.plot(epochs,plot_data[score+"_train"],'o-',label = score+': train')
        plt.plot(epochs,plot_data[score+"_test"],'o--',label = score+': test')
    plt.xlabel('epochs')
    plt.ylabel('Error')
    plt.title(plot_title+" (MLP {} )\n |data| = {}, \n{}-fold cross validation".format(network_size,data_size,k))
    plt.legend(loc='best')
    plt.grid(True)


